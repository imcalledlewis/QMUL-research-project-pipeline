{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93c90c29-7729-4d60-bc09-77dfc7e83674",
   "metadata": {},
   "source": [
    "<h1>Read beta value CSV file and spit  in to training and validation and test sets<h1/>\n",
    "    use hyperimpute to imute tht diffrent splits seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f820121-a421-4415-acf5-84c2b59a4dc0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('filter_betas.csv', sep=',', index_col=0) # for classification problem\n",
    "\n",
    "# Separate the feature data (X) and binary group labels (y)\n",
    "X = df.iloc[:, :-1]  # All columns except the last one\n",
    "y = df.iloc[:, -1]   # Last column (binary group labels)\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into training, testing, and validation sets using a 70/20/10 split\n",
    "X_train, X_test_temp, y_train, y_test_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_temp, y_test_temp, test_size=0.5, random_state=42, stratify=y_test_temp)\n",
    "\n",
    "# Step 1: Separate cases and control data for training set\n",
    "cases_X_train = X_train[y_train == 1]\n",
    "control_X_train = X_train[y_train == 0]\n",
    "\n",
    "# Step 2: Impute the missing values separately for cases and control data in the training set\n",
    "imputers = Imputers()\n",
    "method = \"hyperimpute\"\n",
    "plugin = imputers.get(method)\n",
    "\n",
    "imputed_cases_X_train = plugin.fit_transform(cases_X_train.copy())\n",
    "imputed_control_X_train = plugin.fit_transform(control_X_train.copy())\n",
    "\n",
    "# Concatenate the imputed cases and control data back together in the training set\n",
    "imputed_X_train = pd.concat([pd.DataFrame(imputed_cases_X_train, columns=cases_X_train.columns),\n",
    "                            pd.DataFrame(imputed_control_X_train, columns=control_X_train.columns)])\n",
    "y_train_imputed = np.concatenate((np.ones(imputed_cases_X_train.shape[0]), np.zeros(imputed_control_X_train.shape[0])))\n",
    "\n",
    "# Convert back to DataFrame (optional, if needed) in the training set\n",
    "imputed_X_train = pd.DataFrame(imputed_X_train, columns=X_train.columns)\n",
    "\n",
    "# Step 3: Impute the test and validation data separately\n",
    "imputed_X_test = plugin.fit_transform(X_test.copy())\n",
    "imputed_X_val = plugin.fit_transform(X_val.copy())\n",
    "\n",
    "# Convert back to DataFrame (optional, if needed) in the test and validation sets\n",
    "imputed_X_test = pd.DataFrame(imputed_X_test, columns=X_test.columns)\n",
    "imputed_X_val = pd.DataFrame(imputed_X_val, columns=X_val.columns)\n",
    "\n",
    "# Now imputed_X_train contains the imputed X_train data with the original layout, including feature names and sample IDs\n",
    "# You can use this DataFrame for further analysis or modeling tasks\n",
    "print(imputed_X_train.head())\n",
    "print(imputed_X_test.head())\n",
    "print(imputed_X_val.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08036e0-b580-40d6-8c34-2e63d6ba1478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models['Logistic Regression'] = LogisticRegression(random_state=42)\n",
    "\n",
    "# Support Vector Machines\n",
    "from sklearn.svm import SVC \n",
    "models['Support Vector Machines'] = SVC(random_state=42,)\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models['Decision Trees'] = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models['Random Forest'] = RandomForestClassifier(random_state=42,)\n",
    "\n",
    "# XGboost\n",
    "import xgboost as xgb\n",
    "models['XGboost'] = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "# Gradient Boost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "models['Gradient Boost'] = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "models['Ada boost'] = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "models['Naive Bayes'] = GaussianNB()\n",
    "\n",
    "accuracy, precision, recall, specificity, sensitivity = {}, {}, {}, {}, {}\n",
    "\n",
    "for key in models.keys():\n",
    "    # Fit the classifier model\n",
    "    models[key].fit(imputed_X_train, y_train_imputed)\n",
    "\n",
    "    # Prediction\n",
    "    predictions = models[key].predict(imputed_X_test)\n",
    "\n",
    "    # Calculate Accuracy, Precision, Recall, Specificity, and Sensitivity Metrics\n",
    "    accuracy[key] = accuracy_score(predictions, y_test)\n",
    "    precision[key] = precision_score(predictions, y_test)\n",
    "    recall[key] = recall_score(predictions, y_test)\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # Calculate Specificity and Sensitivity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity[key] = tn / (tn + fp)\n",
    "    sensitivity[key] = tp / (tp + fn)\n",
    "\n",
    "# Create a DataFrame with the metrics\n",
    "df_model = pd.DataFrame(index=models.keys(), columns=['Accuracy', 'Specificity', 'Sensitivity'])\n",
    "df_model['Accuracy'] = accuracy.values()\n",
    "df_model['Specificity'] = specificity.values()\n",
    "df_model['Sensitivity'] = sensitivity.values()\n",
    "\n",
    "#print(df_model)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size before creating the plot\n",
    "ax = df_model.plot.bar(rot=90)\n",
    "\n",
    "# Increase the font size of the legend\n",
    "legend_font_size = 12  # Set your desired font size here\n",
    "ax.legend(ncol=len(models.keys()), bbox_to_anchor=(0, 1), loc='lower left', prop={'size': legend_font_size})\n",
    "plt.xticks(fontsize=8)  # Set the font size of the x-axis tick labels\n",
    "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=12) \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "873d537b-719d-4914-9442-014672ec8a07",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter tuning using GridSearchCV <h1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe38c1-0e6f-40c6-9786-b4fe43be5f8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, n_jobs=-1, max_iter=1000),\n",
    "    'Support Vector Machines': SVC(random_state=42,),\n",
    "    'Decision Trees': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'XGboost': xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, n_jobs=-1),\n",
    "    'Gradient Boost': GradientBoostingClassifier(random_state=42),\n",
    "    'Ada boost': AdaBoostClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "    #add NN\n",
    "}\n",
    "   \n",
    "\n",
    "# Dictionary of the hyperparameter grids for each model\n",
    "# Define a dictionary to store the hyperparameter grids for each model\n",
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [50, 100, 200, 500]\n",
    "    },\n",
    "    'Support Vector Machines': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "        'degree': [2, 3, 4],\n",
    "    },\n",
    "    'Decision Trees': {\n",
    "        'max_depth': [None, 10, 20, 50],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['entropy'],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt'],\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 25, 50, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'bootstrap': [True, False],\n",
    "    },\n",
    "    'XGboost': {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'gamma': [0, 0.1, 0.5, 1],\n",
    "        'subsample': [0.7, 0.8],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "    },\n",
    "    'Gradient Boost': {\n",
    "        'loss': ['deviance', 'exponential'],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'subsample': [0.7, 0.8],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "    },\n",
    "    'Ada boost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.5],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define a dictionary to store the best models and their corresponding hyperparameters\n",
    "best_models = {}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for key in models.keys():\n",
    "    # Define the GridSearchCV with the appropriate model and hyperparameter grid\n",
    "    if key in param_grids:\n",
    "        grid_search = GridSearchCV(models[key], param_grids[key], cv=5, scoring='recall', n_jobs=-1, verbose=1)\n",
    "        # Fit the GridSearchCV to the data\n",
    "        grid_search.fit(imputed_X_train.values, y_train)\n",
    "\n",
    "\n",
    "        # Store the best model and its hyperparameters in the best_models dictionary\n",
    "        best_models[key] = {\n",
    "            'best_model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_\n",
    "        }\n",
    "    else:\n",
    "        # If the model does not require hyperparameter tuning, add it directly to best_models\n",
    "        best_models[key] = {'best_model': models[key], 'best_params': None, 'best_score': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d746d-de80-48cd-a1c4-7efbd938f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_info in best_models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Parameters: {model_info['best_params']}\")\n",
    "    print(f\"Best Score: {model_info['best_score']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d1d76-305e-478f-b8ef-71793cf104dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the evaluation metrics\n",
    "df_best_models = pd.DataFrame(index=best_models.keys(), columns=['Accuracy', 'Specificity', 'Sensitivity'])\n",
    "\n",
    "for key in best_models.keys():\n",
    "    # Get the best model from the best_models dictionary\n",
    "    best_model = best_models[key]['best_model']\n",
    "    \n",
    "    if key == 'Naive Bayes':\n",
    "        # Fit the Naive Bayes model directly to the training data\n",
    "        best_model.fit(imputed_X_train.values, y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    predictions = best_model.predict(imputed_X_val.values)  # Convert to NumPy array\n",
    "\n",
    "    # Calculate Accuracy, Precision, Recall, Specificity, and Sensitivity Metrics\n",
    "    df_best_models.at[key, 'Accuracy'] = accuracy_score(predictions, y_val)\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(y_val, predictions)\n",
    "\n",
    "    # Calculate Specificity and Sensitivity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    df_best_models.at[key, 'Specificity'] = tn / (tn + fp)\n",
    "    df_best_models.at[key, 'Sensitivity'] = tp / (tp + fn)\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = df_best_models.plot.bar(rot=90)\n",
    "\n",
    "print(df_best_models)\n",
    "\n",
    "# Increase the font size of the legend and labels\n",
    "legend_font_size = 12  # Set your desired font size here\n",
    "ax.legend(ncol=len(df_best_models.columns), bbox_to_anchor=(0, 1), loc='lower left', prop={'size': legend_font_size})\n",
    "plt.xticks(fontsize=12)  # Set the font size of the x-axis tick labels\n",
    "plt.yticks(fontsize=12)  # Set the font size of the y-axis tick labels\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1ddce-b04f-452b-852b-40dffa508ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the evaluation metrics\n",
    "df_best_models = pd.DataFrame(index=best_models.keys(), columns=['Accuracy', 'Specificity', 'Sensitivity'])\n",
    "\n",
    "for key in best_models.keys():\n",
    "    # Get the best model from the best_models dictionary\n",
    "    best_model = best_models[key]['best_model']\n",
    "    \n",
    "    if key == 'Naive Bayes':\n",
    "        # Fit the Naive Bayes model directly to the training data\n",
    "        best_model.fit(imputed_X_train.values, y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    predictions = best_model.predict(imputed_X_test.values)  # Convert to NumPy array\n",
    "\n",
    "    # Calculate Accuracy, Precision, Recall, Specificity, and Sensitivity Metrics\n",
    "    df_best_models.at[key, 'Accuracy'] = accuracy_score(predictions, y_test)\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # Calculate Specificity and Sensitivity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    df_best_models.at[key, 'Specificity'] = tn / (tn + fp)\n",
    "    df_best_models.at[key, 'Sensitivity'] = tp / (tp + fn)\n",
    "\n",
    "    \n",
    "print(df_best_models)\n",
    "# Set the figure size before creating the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = df_best_models.plot.bar(rot=90)\n",
    "\n",
    "# Increase the font size of the legend and labels\n",
    "legend_font_size = 12  # Set your desired font size here\n",
    "ax.legend(ncol=len(df_best_models.columns), bbox_to_anchor=(0, 1), loc='lower left', prop={'size': legend_font_size})\n",
    "plt.xticks(fontsize=8)  # Set the font size of the x-axis tick labels\n",
    "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=12)  # Set the font size of the y-axis tick labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c46cc55-a652-4956-ae05-bc5ea6636e33",
   "metadata": {},
   "source": [
    "<h1>Chosen models for hand tuning <h1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0a4d8-0729-4c7b-af7e-cb046aa85c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models['Logistic Regression'] = LogisticRegression(C=7, penalty='l1', solver='liblinear', max_iter=120)\n",
    "\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models['Decision Trees'] =  DecisionTreeClassifier(random_state=42, criterion ='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "models['Ada boost'] = AdaBoostClassifier(n_estimators=160, random_state=42, learning_rate=0.5,)\n",
    "\n",
    "\n",
    "accuracy, precision, recall, specificity, sensitivity = {}, {}, {}, {}, {}\n",
    "\n",
    "for key in models.keys():\n",
    "    # Fit the classifier model\n",
    "    models[key].fit(imputed_X_train, y_train)\n",
    "\n",
    "    # Prediction\n",
    "    predictions = models[key].predict(imputed_X_test)\n",
    "\n",
    "    # Calculate Accuracy, Precision, Recall, Specificity, and Sensitivity Metrics\n",
    "    accuracy[key] = accuracy_score(predictions, y_test)\n",
    "    precision[key] = precision_score(predictions, y_test)\n",
    "    recall[key] = recall_score(predictions, y_test)\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # Calculate Specificity and Sensitivity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity[key] = tn / (tn + fp)\n",
    "    sensitivity[key] = tp / (tp + fn)\n",
    "\n",
    "# Create a DataFrame with the metrics\n",
    "df_model = pd.DataFrame(index=models.keys(), columns=['Accuracy', 'Specificity', 'Sensitivity'])\n",
    "df_model['Accuracy'] = accuracy.values()\n",
    "df_model['Specificity'] = specificity.values()\n",
    "df_model['Sensitivity'] = sensitivity.values()\n",
    "\n",
    "print(df_model)\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = df_model.plot.bar(rot=90)\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Increase the font size of the legend\n",
    "legend_font_size = 12  # Set your desired font size here\n",
    "ax.legend(ncol=len(models.keys()), bbox_to_anchor=(0, 1), loc='lower left', prop={'size': legend_font_size})\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f47af-3e82-4489-985b-8a38b24502b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Logistic Regression model with specific hyperparameters\n",
    "LR = LogisticRegression(C=7, penalty='l1', solver='liblinear', max_iter=120)\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "LR.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_prob = LR.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Choose a specific threshold for classification\n",
    "chosen_threshold = 0.6651\n",
    "\n",
    "# Make binary predictions based on the chosen threshold\n",
    "y_pred_binary = (y_prob >= chosen_threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy using the accuracy_score function from scikit-learn\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate the confusion matrix using the confusion_matrix function from scikit-learn\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate sensitivity and specificity from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "TP = conf_matrix[1, 1]\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print the model metrics at the chosen threshold\n",
    "print(\"Model Metrics at Chosen Threshold:\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds\n",
    "thresholds = np.arange(0.001, 1.0, 0.001)\n",
    "sensitivity_values = []\n",
    "specificity_values = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary = (y_prob >= threshold).astype(int)\n",
    "    TP = np.sum((y_test == 1) & (y_pred_binary == 1))\n",
    "    FP = np.sum((y_test == 0) & (y_pred_binary == 1))\n",
    "    TN = np.sum((y_test == 0) & (y_pred_binary == 0))\n",
    "    FN = np.sum((y_test == 1) & (y_pred_binary == 0))\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    sensitivity_values.append(sensitivity)\n",
    "    specificity_values.append(specificity)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, sensitivity_values, label='Sensitivity')\n",
    "plt.plot(thresholds, specificity_values, label='Specificity')\n",
    "plt.axvline(x=chosen_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity and Specificity at Different Probability Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de9cfe-b233-4cf9-bb87-49c027a849d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Logistic Regression model with specific hyperparameters\n",
    "LR = LogisticRegression(C=7, penalty='l1', solver='liblinear', max_iter=120)\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "LR.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_prob = LR.predict_proba(imputed_X_val.values)[:, 1]\n",
    "\n",
    "# Choose a specific threshold for classification\n",
    "chosen_threshold = 0.6648\n",
    "\n",
    "\n",
    "# Make binary predictions based on the chosen threshold\n",
    "y_pred_binary = (y_prob >= chosen_threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy using the accuracy_score function from scikit-learn\n",
    "accuracy = accuracy_score(y_val, y_pred_binary)\n",
    "\n",
    "# Calculate the confusion matrix using the confusion_matrix function from scikit-learn\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_binary)\n",
    "\n",
    "# Calculate sensitivity and specificity from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "TP = conf_matrix[1, 1]\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print the model metrics at the chosen threshold\n",
    "print(\"Model Metrics at Chosen Threshold:\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds\n",
    "thresholds = np.arange(0.001, 1.0, 0.001)\n",
    "sensitivity_values = []\n",
    "specificity_values = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary = (y_prob >= threshold).astype(int)\n",
    "    TP = np.sum((y_val == 1) & (y_pred_binary == 1))\n",
    "    FP = np.sum((y_val == 0) & (y_pred_binary == 1))\n",
    "    TN = np.sum((y_val == 0) & (y_pred_binary == 0))\n",
    "    FN = np.sum((y_val == 1) & (y_pred_binary == 0))\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    sensitivity_values.append(sensitivity)\n",
    "    specificity_values.append(specificity)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, sensitivity_values, label='Sensitivity')\n",
    "plt.plot(thresholds, specificity_values, label='Specificity')\n",
    "plt.axvline(x=chosen_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity and Specificity at Different Probability Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9f759-a5ef-4484-ab47-235230af54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Logistic Regression model with specific hyperparameters\n",
    "LR = LogisticRegression(C=7, penalty='l1', solver='liblinear', max_iter=120)\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "LR.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_prob_val = LR.predict_proba(imputed_X_val.values)[:, 1]\n",
    "y_prob_train = LR.predict_proba(imputed_X_train.values)[:, 1]\n",
    "y_prob_test = LR.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Choose a specific threshold for classification\n",
    "chosen_threshold = 0.6645\n",
    "# Make binary predictions based on the chosen threshold for validation data\n",
    "y_pred_binary_val = (y_prob_val >= chosen_threshold).astype(int)\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for training data\n",
    "y_pred_binary_train = (y_prob_train >= chosen_threshold).astype(int)\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for testing data\n",
    "y_pred_binary_test = (y_prob_test >= chosen_threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy for validation, training, and testing data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_binary_val)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_binary_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_binary_test)\n",
    "\n",
    "# Calculate the confusion matrix for validation, training, and testing data\n",
    "conf_matrix_val = confusion_matrix(y_val, y_pred_binary_val)\n",
    "conf_matrix_train = confusion_matrix(y_train, y_pred_binary_train)\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_binary_test)\n",
    "\n",
    "# Calculate sensitivity and specificity from the confusion matrix for validation, training, and testing data\n",
    "TN_val, FP_val, FN_val, TP_val = conf_matrix_val.ravel()\n",
    "sensitivity_val = TP_val / (TP_val + FN_val)\n",
    "specificity_val = TN_val / (TN_val + FP_val)\n",
    "\n",
    "TN_train, FP_train, FN_train, TP_train = conf_matrix_train.ravel()\n",
    "sensitivity_train = TP_train / (TP_train + FN_train)\n",
    "specificity_train = TN_train / (TN_train + FP_train)\n",
    "\n",
    "TN_test, FP_test, FN_test, TP_test = conf_matrix_test.ravel()\n",
    "sensitivity_test = TP_test / (TP_test + FN_test)\n",
    "specificity_test = TN_test / (TN_test + FP_test)\n",
    "\n",
    "\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Training Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
    "print(f\"Training Sensitivity: {sensitivity_train:.3f}\")\n",
    "print(f\"Training Specificity: {specificity_train:.3f}\")\n",
    "\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Testing Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Testing Accuracy: {accuracy_test:.3f}\")\n",
    "print(f\"Testing Sensitivity: {sensitivity_test:.3f}\")\n",
    "print(f\"Testing Specificity: {specificity_test:.3f}\")\n",
    "\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for training data\n",
    "sensitivity_values_train = []\n",
    "specificity_values_train = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_train = (y_prob_train >= threshold).astype(int)\n",
    "    TP_train = np.sum((y_train == 1) & (y_pred_binary_train == 1))\n",
    "    FP_train = np.sum((y_train == 0) & (y_pred_binary_train == 1))\n",
    "    TN_train = np.sum((y_train == 0) & (y_pred_binary_train == 0))\n",
    "    FN_train = np.sum((y_train == 1) & (y_pred_binary_train == 0))\n",
    "    sensitivity_train = TP_train / (TP_train + FN_train)\n",
    "    specificity_train = TN_train / (TN_train + FP_train)\n",
    "    sensitivity_values_train.append(sensitivity_train)\n",
    "    specificity_values_train.append(specificity_train)\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for testing data\n",
    "sensitivity_values_test = []\n",
    "specificity_values_test = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_test = (y_prob_test >= threshold).astype(int)\n",
    "    TP_test = np.sum((y_test == 1) & (y_pred_binary_test == 1))\n",
    "    FP_test = np.sum((y_test == 0) & (y_pred_binary_test == 1))\n",
    "    TN_test = np.sum((y_test == 0) & (y_pred_binary_test == 0))\n",
    "    FN_test = np.sum((y_test == 1) & (y_pred_binary_test == 0))\n",
    "    sensitivity_test = TP_test / (TP_test + FN_test)\n",
    "    specificity_test = TN_test / (TN_test + FP_test)\n",
    "    sensitivity_values_test.append(sensitivity_test)\n",
    "    specificity_values_test.append(specificity_test)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, sensitivity_values_train, label='Training Sensitivity', color='dimgrey', linestyle='dashed')\n",
    "plt.plot(thresholds, specificity_values_train, label='Training Specificity', color='darkgrey', linestyle='dashed')\n",
    "plt.plot(thresholds, sensitivity_values_test, label='Testing Sensitivity', color='darkorange', )\n",
    "plt.plot(thresholds, specificity_values_test, label='Testing Specificity', color='navy', )\n",
    "plt.axvline(x=chosen_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity and Specificity at Different Probability Thresholds (LogisticRegression)',size=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c5fe0-425c-411b-9d05-73fbc9b96e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an AdaBoost model with specific hyperparameters\n",
    "ada_model = AdaBoostClassifier(n_estimators=160, random_state=42, learning_rate=0.5, )\n",
    "\n",
    "# Fit the AdaBoost model to the training data\n",
    "ada_model.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_prob_val = ada_model.predict_proba(imputed_X_val.values)[:, 1]\n",
    "y_prob_train = ada_model.predict_proba(imputed_X_train.values)[:, 1]\n",
    "y_prob_test = ada_model.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Choose a specific threshold for classification\n",
    "chosen_threshold = 0.5\n",
    "\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for validation data\n",
    "y_pred_binary_val = (y_prob_val >= chosen_threshold).astype(int)\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for training data\n",
    "y_pred_binary_train = (y_prob_train >= chosen_threshold).astype(int)\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for testing data\n",
    "y_pred_binary_test = (y_prob_test >= chosen_threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy for validation, training, and testing data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_binary_val)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_binary_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_binary_test)\n",
    "\n",
    "# Calculate the confusion matrix for validation, training, and testing data\n",
    "conf_matrix_val = confusion_matrix(y_val, y_pred_binary_val)\n",
    "conf_matrix_train = confusion_matrix(y_train, y_pred_binary_train)\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_binary_test)\n",
    "\n",
    "# Calculate sensitivity and specificity from the confusion matrix for validation, training, and testing data\n",
    "TN_val, FP_val, FN_val, TP_val = conf_matrix_val.ravel()\n",
    "sensitivity_val = TP_val / (TP_val + FN_val)\n",
    "specificity_val = TN_val / (TN_val + FP_val)\n",
    "\n",
    "TN_train, FP_train, FN_train, TP_train = conf_matrix_train.ravel()\n",
    "sensitivity_train = TP_train / (TP_train + FN_train)\n",
    "specificity_train = TN_train / (TN_train + FP_train)\n",
    "\n",
    "TN_test, FP_test, FN_test, TP_test = conf_matrix_test.ravel()\n",
    "sensitivity_test = TP_test / (TP_test + FN_test)\n",
    "specificity_test = TN_test / (TN_test + FP_test)\n",
    "\n",
    "# Print the model metrics at the chosen threshold for validation, training, and testing data\n",
    "\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Training Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
    "print(f\"Training Sensitivity: {sensitivity_train:.3f}\")\n",
    "print(f\"Training Specificity: {specificity_train:.3f}\")\n",
    "\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Testing Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Testing Accuracy: {accuracy_test:.3f}\")\n",
    "print(f\"Testing Sensitivity: {sensitivity_test:.3f}\")\n",
    "print(f\"Testing Specificity: {specificity_test:.3f}\")\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for validation data\n",
    "thresholds = np.arange(0.001, 1.0, 0.001)\n",
    "sensitivity_values_val = []\n",
    "specificity_values_val = []\n",
    "\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for training data\n",
    "sensitivity_values_train = []\n",
    "specificity_values_train = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_train = (y_prob_train >= threshold).astype(int)\n",
    "    TP_train = np.sum((y_train == 1) & (y_pred_binary_train == 1))\n",
    "    FP_train = np.sum((y_train == 0) & (y_pred_binary_train == 1))\n",
    "    TN_train = np.sum((y_train == 0) & (y_pred_binary_train == 0))\n",
    "    FN_train = np.sum((y_train == 1) & (y_pred_binary_train == 0))\n",
    "    sensitivity_train = TP_train / (TP_train + FN_train)\n",
    "    specificity_train = TN_train / (TN_train + FP_train)\n",
    "    sensitivity_values_train.append(sensitivity_train)\n",
    "    specificity_values_train.append(specificity_train)\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for testing data\n",
    "sensitivity_values_test = []\n",
    "specificity_values_test = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_test = (y_prob_test >= threshold).astype(int)\n",
    "    TP_test = np.sum((y_test == 1) & (y_pred_binary_test == 1))\n",
    "    FP_test = np.sum((y_test == 0) & (y_pred_binary_test == 1))\n",
    "    TN_test = np.sum((y_test == 0) & (y_pred_binary_test == 0))\n",
    "    FN_test = np.sum((y_test == 1) & (y_pred_binary_test == 0))\n",
    "    sensitivity_test = TP_test / (TP_test + FN_test)\n",
    "    specificity_test = TN_test / (TN_test + FP_test)\n",
    "    sensitivity_values_test.append(sensitivity_test)\n",
    "    specificity_values_test.append(specificity_test)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, sensitivity_values_train, label='Training Sensitivity', color='dimgrey', linestyle='dashed')\n",
    "plt.plot(thresholds, specificity_values_train, label='Training Specificity', color='darkgrey', linestyle='dashed')\n",
    "plt.plot(thresholds, sensitivity_values_test, label='Testing Sensitivity', color='darkorange', )\n",
    "plt.plot(thresholds, specificity_values_test, label='Testing Specificity', color='navy', )\n",
    "plt.axvline(x=chosen_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity and Specificity at Different Probability Thresholds (AdaBoost)',size=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fe4b6-be52-49e1-88e5-938dfedeb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Decision Tree model with specific hyperparameters\n",
    "DT = DecisionTreeClassifier( random_state=42, criterion ='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "# Fit the Decision Tree model to the training data\n",
    "DT.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_prob_val = DT.predict_proba(imputed_X_val.values)[:, 1]\n",
    "y_prob_train = DT.predict_proba(imputed_X_train.values)[:, 1]\n",
    "y_prob_test = DT.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Choose a specific threshold for classification\n",
    "chosen_threshold = 0.5\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for validation data\n",
    "y_pred_binary_val = (y_prob_val >= chosen_threshold).astype(int)\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for training data\n",
    "y_pred_binary_train = (y_prob_train >= chosen_threshold).astype(int)\n",
    "\n",
    "# Make binary predictions based on the chosen threshold for testing data\n",
    "y_pred_binary_test = (y_prob_test >= chosen_threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy for validation, training, and testing data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_binary_val)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_binary_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_binary_test)\n",
    "\n",
    "# Calculate the confusion matrix for validation, training, and testing data\n",
    "conf_matrix_val = confusion_matrix(y_val, y_pred_binary_val)\n",
    "conf_matrix_train = confusion_matrix(y_train, y_pred_binary_train)\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_binary_test)\n",
    "\n",
    "# Calculate sensitivity and specificity from the confusion matrix for validation, training, and testing data\n",
    "TN_val, FP_val, FN_val, TP_val = conf_matrix_val.ravel()\n",
    "sensitivity_val = TP_val / (TP_val + FN_val)\n",
    "specificity_val = TN_val / (TN_val + FP_val)\n",
    "\n",
    "TN_train, FP_train, FN_train, TP_train = conf_matrix_train.ravel()\n",
    "sensitivity_train = TP_train / (TP_train + FN_train)\n",
    "specificity_train = TN_train / (TN_train + FP_train)\n",
    "\n",
    "TN_test, FP_test, FN_test, TP_test = conf_matrix_test.ravel()\n",
    "sensitivity_test = TP_test / (TP_test + FN_test)\n",
    "specificity_test = TN_test / (TN_test + FP_test)\n",
    "\n",
    "# Print the model metrics at the chosen threshold for validation, training, and testing data\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Training Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
    "print(f\"Training Sensitivity: {sensitivity_train:.3f}\")\n",
    "print(f\"Training Specificity: {specificity_train:.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Testing Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Testing Accuracy: {accuracy_test:.3f}\")\n",
    "print(f\"Testing Sensitivity: {sensitivity_test:.3f}\")\n",
    "print(f\"Testing Specificity: {specificity_test:.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\nModel Metrics at Chosen Threshold (Validation Data):\")\n",
    "print(f\"Threshold: {chosen_threshold}\")\n",
    "print(f\"Validation Accuracy: {accuracy_val:.3f}\")\n",
    "print(f\"Validation Sensitivity: {sensitivity_val:.3f}\")\n",
    "print(f\"Validation Specificity: {specificity_val:.3f}\")\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for validation data\n",
    "thresholds = np.arange(0.001, 1.0, 0.001)\n",
    "sensitivity_values_val = []\n",
    "specificity_values_val = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_val = (y_prob_val >= threshold).astype(int)\n",
    "    TP_val = np.sum((y_val == 1) & (y_pred_binary_val == 1))\n",
    "    FP_val = np.sum((y_val == 0) & (y_pred_binary_val == 1))\n",
    "    TN_val = np.sum((y_val == 0) & (y_pred_binary_val == 0))\n",
    "    FN_val = np.sum((y_val == 1) & (y_pred_binary_val == 0))\n",
    "    sensitivity_val = TP_val / (TP_val + FN_val)\n",
    "    specificity_val = TN_val / (TN_val + FP_val)\n",
    "    sensitivity_values_val.append(sensitivity_val)\n",
    "    specificity_values_val.append(specificity_val)\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for training data\n",
    "sensitivity_values_train = []\n",
    "specificity_values_train = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_train = (y_prob_train >= threshold).astype(int)\n",
    "    TP_train = np.sum((y_train == 1) & (y_pred_binary_train == 1))\n",
    "    FP_train = np.sum((y_train == 0) & (y_pred_binary_train == 1))\n",
    "    TN_train = np.sum((y_train == 0) & (y_pred_binary_train == 0))\n",
    "    FN_train = np.sum((y_train == 1) & (y_pred_binary_train == 0))\n",
    "    sensitivity_train = TP_train / (TP_train + FN_train)\n",
    "    specificity_train = TN_train / (TN_train + FP_train)\n",
    "    sensitivity_values_train.append(sensitivity_train)\n",
    "    specificity_values_train.append(specificity_train)\n",
    "\n",
    "# Plot sensitivity and specificity at different probability thresholds for testing data\n",
    "sensitivity_values_test = []\n",
    "specificity_values_test = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_binary_test = (y_prob_test >= threshold).astype(int)\n",
    "    TP_test = np.sum((y_test == 1) & (y_pred_binary_test == 1))\n",
    "    FP_test = np.sum((y_test == 0) & (y_pred_binary_test == 1))\n",
    "    TN_test = np.sum((y_test == 0) & (y_pred_binary_test == 0))\n",
    "    FN_test = np.sum((y_test == 1) & (y_pred_binary_test == 0))\n",
    "    sensitivity_test = TP_test / (TP_test + FN_test)\n",
    "    specificity_test = TN_test / (TN_test + FP_test)\n",
    "    sensitivity_values_test.append(sensitivity_test)\n",
    "    specificity_values_test.append(specificity_test)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, sensitivity_values_train, label='Training Sensitivity', color='dimgrey', linestyle='dashed')\n",
    "plt.plot(thresholds, specificity_values_train, label='Training Specificity', color='darkgrey', linestyle='dashed')\n",
    "plt.plot(thresholds, sensitivity_values_test, label='Testing Sensitivity', color='darkorange', )\n",
    "plt.plot(thresholds, specificity_values_test, label='Test Specificity', color='navy', )\n",
    "plt.axvline(x=chosen_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity and Specificity at Different Probability Thresholds (DecisionTree)',size=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d1059-84d5-4fc8-acb7-18c1d5b03826",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# Create a Decision Tree model with specific hyperparameters\n",
    "DT = DecisionTreeClassifier( random_state=42, criterion ='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "# Fit the Decision Tree model to the training data\n",
    "DT.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Step 1: Make predictions on the validation data using the Decision Tree model\n",
    "y_pred_proba_val = DT.predict_proba(imputed_X_val.values)[:, 1]\n",
    "\n",
    "# Step 2: Calculate the ROC curve for the validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 3: Calculate the AUC (Area Under the Curve) score for the validation data\n",
    "auc_score_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 4: Make predictions on the testing data using the Decision Tree model\n",
    "#y_pred_proba_test = DT.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Step 5: Calculate the ROC curve for the testing data\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 6: Calculate the AUC (Area Under the Curve) score for the testing data\n",
    "#auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 7: Plot the ROC curves for both validation and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'Testing ROC curve (AUC = {auc_score_test:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Decision Tree Receiver Operating Characteristic (ROC) Curves \\n With reduced features')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386632b-725c-446d-bb80-f84c2da991b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Create an AdaBoost model with specific hyperparameters\n",
    "ada_model = AdaBoostClassifier(n_estimators=160, random_state=42, learning_rate=0.5, )\n",
    "\n",
    "# Fit the AdaBoost model to the training data\n",
    "ada_model.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Step 1: Make predictions on the validation data using the AdaBoostClassifier model\n",
    "y_pred_proba_val = ada_model.predict_proba(imputed_X_val.values)[:, 1]\n",
    "\n",
    "# Step 2: Calculate the ROC curve for the validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 3: Calculate the AUC (Area Under the Curve) score for the validation data\n",
    "auc_score_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 4: Make predictions on the testing data using the AdaBoostClassifier model\n",
    "y_pred_proba_test = ada_model.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Step 5: Calculate the ROC curve for the testing data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 6: Calculate the AUC (Area Under the Curve) score for the testing data\n",
    "auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 7: Plot the ROC curves for both the validation and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'Testing ROC curve (AUC = {auc_score_val:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=18)  # Increase font size for x-axis label\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=18)   # Increase font size for y-axis label\n",
    "plt.title('Adaboost Receiver Operating Characteristic (ROC) Curve', fontsize=20, y=1.1 )  # Increase font size for title\n",
    "plt.legend(loc='lower right', fontsize=16)  # Increase font size for legend\n",
    "\n",
    "plt.xticks(fontsize=16)  # Increase font size for x-axis ticks\n",
    "plt.yticks(fontsize=16)  # Increase font size for y-axis ticks\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65995e79-2fd6-46fe-a8d2-0e43c54b40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Create a Logistic Regression model with specific hyperparameters\n",
    "LR = LogisticRegression(C=7, penalty='l1', solver='liblinear', max_iter=120)\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "LR.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Step 1: Make predictions on the validation data using the LogisticRegression model\n",
    "y_pred_proba_val = LR.predict_proba(imputed_X_val.values)[:, 1]\n",
    "\n",
    "# Step 2: Make predictions on the testing data using the LogisticRegression model\n",
    "y_pred_proba_test = LR.predict_proba(imputed_X_test.values)[:, 1]\n",
    "\n",
    "# Step 3: Calculate the ROC curve for the validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_proba_val)\n",
    "# Calculate the AUC (Area Under the Curve) score for the validation data\n",
    "auc_score_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "\n",
    "'''# Step 4: Calculate the ROC curve for the testing data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n",
    "# Calculate the AUC (Area Under the Curve) score for the testing data\n",
    "auc_score_test = roc_auc_score(y_test, y_pred_proba_test)'''\n",
    "\n",
    "# Step 5: Plot the ROC curve for both validation and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'Testing ROC curve (AUC = {auc_score_val:.2f})')\n",
    "#plt.plot(fpr_test, tpr_test, color='green', lw=2, label=f'Testing ROC curve (AUC = {auc_score_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=18)  # Increase font size for x-axis label\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=18)   # Increase font size for y-axis label\n",
    "plt.title('LogisticRegression Receiver Operating Characteristic (ROC) Curve', fontsize=20, y=1.1 )  # Increase font size for title\n",
    "plt.legend(loc='lower right', fontsize=16)  # Increase font size for legend\n",
    "\n",
    "plt.xticks(fontsize=16)  # Increase font size for x-axis ticks\n",
    "plt.yticks(fontsize=16)  # Increase font size for y-axis ticks\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8f127-b36a-4c63-b1ff-fad01505388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the features_counts DataFrame from the CSV file\n",
    "features_counts = pd.read_csv('/data/home/bt22880/WGBS_TE_pipeline/TensorFlow/Feature_selection/feature_counts_rfecv_boruta_lasso.csv', sep=',')\n",
    "cgp_list = features_counts['Cpg'].tolist()\n",
    "#print(cgp_list)\n",
    "\n",
    "# Keep only the features that are present in the cgp_list\n",
    "filterd_X_val = imputed_X_val[imputed_X_val.columns.intersection(cgp_list)]\n",
    "filterd_X_test = imputed_X_test[imputed_X_test.columns.intersection(cgp_list)]\n",
    "filterd_X_train = imputed_X_train[imputed_X_train.columns.intersection(cgp_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a5bfd-39d6-45d6-b5bb-91e857fa2087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# Create a Decision Tree model with specific hyperparameters\n",
    "DT =  DecisionTreeClassifier( random_state=42, criterion ='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "# Fit the Decision Tree model to the training data\n",
    "DT.fit(filterd_X_train, y_train)\n",
    "\n",
    "# Step 1: Make predictions on the validation data using the Decision Tree model\n",
    "y_pred_proba_val = DT.predict_proba(filterd_X_val.values)[:, 1]\n",
    "\n",
    "# Step 2: Calculate the ROC curve for the validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 3: Calculate the AUC (Area Under the Curve) score for the validation data\n",
    "auc_score_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 4: Make predictions on the testing data using the Decision Tree model\n",
    "#y_pred_proba_test = DT.predict_proba(filterd_X_test.values)[:, 1]\n",
    "\n",
    "# Step 5: Calculate the ROC curve for the testing data\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 6: Calculate the AUC (Area Under the Curve) score for the testing data\n",
    "#auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 7: Plot the ROC curves for both validation and testing data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'Testing ROC curve (AUC = {auc_score_val:.2f})')\n",
    "#plt.plot(fpr_test, tpr_test, color='green', lw=2, label=f'Testing ROC curve (AUC = {auc_score_test:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=18)  # Increase font size for x-axis label\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=18)   # Increase font size for y-axis label\n",
    "plt.title('Decision Tree Receiver Operating Characteristic (ROC) Curves', fontsize=20, y=1.1 )  # Increase font size for title\n",
    "plt.legend(loc='lower right', fontsize=16)  # Increase font size for legend\n",
    "\n",
    "plt.xticks(fontsize=16)  # Increase font size for x-axis ticks\n",
    "plt.yticks(fontsize=16)  # Increase font size for y-axis ticks\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92dcc2-5f6d-465a-9c8e-3d84ac340383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the filterd_X_train, y_train, filterd_X_val, y_val, filterd_X_test, and y_test\n",
    "\n",
    "# Create a Decision Tree model with specific hyperparameters\n",
    "DT =  DecisionTreeClassifier( random_state=42, criterion ='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "# Fit the Decision Tree model to the training data\n",
    "DT.fit(filterd_X_train, y_train)\n",
    "\n",
    "# Step 1: Make predictions on the validation data using the Decision Tree model\n",
    "y_pred_proba_val = DT.predict_proba(filterd_X_val.values)[:, 1]\n",
    "\n",
    "# Step 2: Calculate the ROC curve for the validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 3: Calculate the AUC (Area Under the Curve) score for the validation data\n",
    "auc_score_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "\n",
    "# Step 4: Make predictions on the testing data using the Decision Tree model\n",
    "#y_pred_proba_test = DT.predict_proba(filterd_X_test.values)[:, 1]\n",
    "\n",
    "# Step 5: Calculate the ROC curve for the testing data\n",
    "#fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 6: Calculate the AUC (Area Under the Curve) score for the testing data\n",
    "#auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "# Step 7: Plot the ROC curves for both validation and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'Testing ROC curve (AUC = {auc_score_val:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=18)  # Increase font size for x-axis label\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=18)   # Increase font size for y-axis label\n",
    "plt.title('Decision Tree Receiver Operating Characteristic (ROC) Curves\\n With reduced features', fontsize=20, y=1.1 )  # Increase font size for title\n",
    "plt.legend(loc='lower right', fontsize=16)  # Increase font size for legend\n",
    "\n",
    "plt.xticks(fontsize=16)  # Increase font size for x-axis ticks\n",
    "plt.yticks(fontsize=16)  # Increase font size for y-axis ticks\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002188cd-87f0-4ea0-97be-8851faf66650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the individual models\n",
    "ada_model = AdaBoostClassifier(n_estimators=160, random_state=42, learning_rate=0.5)\n",
    "DT = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "LR = LogisticRegression(C=7, penalty='l1', solver='liblinear', max_iter=120)\n",
    "\n",
    "# Create the Voting Classifier by combining the models\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('ADA', ada_model),\n",
    "    ('DT', DT),\n",
    "    ('LR', LR)\n",
    "], voting='soft', weights=[1.0, 1.5, 0.5])\n",
    "\n",
    "# Train the ensemble model on the training data\n",
    "ensemble_model.fit(filterd_X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = ensemble_model.predict(filterd_X_val)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Calculate specificity and sensitivity from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "TP = conf_matrix[1, 1]\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "\n",
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_probs = ensemble_model.predict_proba(filterd_X_val)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_probs)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) score\n",
    "auc = roc_auc_score(y_val, y_probs)\n",
    "\n",
    "print(\"Ensemble Model Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Ensemble Model (AUC = {auc:.2f})', color='blue', lw=2,)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=18)  # Increase font size for x-axis label\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=18)   # Increase font size for y-axis label\n",
    "plt.title('Ensemble Model Receiver Operating Characteristic (ROC) Curves \\n With reduced features',  fontsize=20, y=1.1 )\n",
    "plt.legend(loc='lower right', fontsize=16)  # Increase font size for legend\n",
    "plt.xticks(fontsize=16)  # Increase font size for x-axis ticks\n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
